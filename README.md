This evaluation pipeline is part of the reproduction results for the dissertation "Exploring Grammatical Constraints in Large Language Models" by David Silva.
It reproduces some of the results from the "BabyLM challenge" (Hu et al. 2024; Wardstad et al., 2024).

It is forked from: https://github.com/babylm/evaluation-pipeline-2024

To start the evaluation, run the respective files 'evaluation_model.py'.
Install all the necessary packages as they are required.
All results are saved in the folder: 'results'.

References:

Hu, M. Y., Mueller, A., Ross, C., Williams, A., Linzen, T., Zhuang, C., Cotterell, R., Choshen, L., Warstadt, A., & Wilcox, E. G. (2024). Findings of the Second BabyLM Challenge: Sample-efficient pretraining on developmentally plausible corpora [Preprint]. arXiv. https://arxiv.org/abs/2412.05149

Warstadt, A., Mueller, A., Choshen, L., Wilcox, E. G., Zhuang, C., Williams, A., Cotterell, R., & Linzen, T. (2023b). Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning. ACL Anthology. https://aclanthology.org/volumes/2023.conll-babylm/

The reproduction of results with prompting methods is availabe in a Colab file: https://colab.research.google.com/drive/1xKxk4nczHxpcmeIyvCikSE74ZAiGwYoF
Needs an openAI key to be used.
