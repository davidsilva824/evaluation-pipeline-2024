This evaluation pipeline is part of the reproduction results for the dissertation "Exploring Grammatical Constraints in Large Language Models" by David Silva.
It reproduces some of the results from the "BabyLM challenge".

It is forked from: babylm/evaluation-pipeline-2024

To start the evaluation, run the respective files 'evaluation_model.py'. 
All results are saved in the folder: 'results'.

The reproduction of results with prompting methods from Hu et al.(2024) is availabe in a Colab file: https://colab.research.google.com/drive/1xKxk4nczHxpcmeIyvCikSE74ZAiGwYoF
You will need an openAI key to use it.
